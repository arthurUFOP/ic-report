{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ANyVIN9LBls3",
        "outputId": "c2c65b01-6fee-4a4c-ab0a-9cfe7f15d8e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'TinyML-Contest-Solution'...\n",
            "remote: Enumerating objects: 30547, done.\u001b[K\n",
            "remote: Counting objects: 100% (2/2), done.\u001b[K\n",
            "remote: Total 30547 (delta 0), reused 2 (delta 0), pack-reused 30545\u001b[K\n",
            "Receiving objects: 100% (30547/30547), 153.75 MiB | 7.53 MiB/s, done.\n",
            "Resolving deltas: 100% (84/84), done.\n",
            "Updating files: 100% (30513/30513), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/GATECH-EIC/TinyML-Contest-Solution.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PM3VMgJcCy0A",
        "outputId": "6bd0a15f-9d9e-480f-ecf4-b02948780c42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/TinyML-Contest-Solution/Training\n"
          ]
        }
      ],
      "source": [
        "%cd TinyML-Contest-Solution/Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O0ZeObFIC-v8",
        "outputId": "3ff0f3b3-f1ac-431b-fd1a-794ae41fad43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start imports\n",
            "end imports\n",
            "passed here\n",
            "Reading data...\n",
            "200 entries read...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-40b512373d09>:44: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  datamat = np.arange(row, dtype=np.float)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "400 entries read...\n",
            "600 entries read...\n",
            "800 entries read...\n",
            "1000 entries read...\n",
            "1200 entries read...\n",
            "1400 entries read...\n",
            "1600 entries read...\n",
            "1800 entries read...\n",
            "2000 entries read...\n",
            "2200 entries read...\n",
            "2400 entries read...\n",
            "2600 entries read...\n",
            "2800 entries read...\n",
            "3000 entries read...\n",
            "3200 entries read...\n",
            "3400 entries read...\n",
            "3600 entries read...\n",
            "3800 entries read...\n",
            "4000 entries read...\n",
            "4200 entries read...\n",
            "4400 entries read...\n",
            "4600 entries read...\n",
            "4800 entries read...\n",
            "5000 entries read...\n",
            "5200 entries read...\n",
            "5400 entries read...\n",
            "5600 entries read...\n",
            "5800 entries read...\n",
            "6000 entries read...\n",
            "6200 entries read...\n",
            "6400 entries read...\n",
            "6600 entries read...\n",
            "6800 entries read...\n",
            "7000 entries read...\n",
            "7200 entries read...\n",
            "7400 entries read...\n",
            "7600 entries read...\n",
            "7800 entries read...\n",
            "8000 entries read...\n",
            "8200 entries read...\n",
            "8400 entries read...\n",
            "8600 entries read...\n",
            "8800 entries read...\n",
            "9000 entries read...\n",
            "9200 entries read...\n",
            "9400 entries read...\n",
            "9600 entries read...\n",
            "9800 entries read...\n",
            "10000 entries read...\n",
            "10200 entries read...\n",
            "10400 entries read...\n",
            "10600 entries read...\n",
            "10800 entries read...\n",
            "11000 entries read...\n",
            "11200 entries read...\n",
            "11400 entries read...\n",
            "11600 entries read...\n",
            "11800 entries read...\n",
            "12000 entries read...\n",
            "12200 entries read...\n",
            "12400 entries read...\n",
            "12600 entries read...\n",
            "12800 entries read...\n",
            "13000 entries read...\n",
            "13200 entries read...\n",
            "13400 entries read...\n",
            "13600 entries read...\n",
            "13800 entries read...\n",
            "14000 entries read...\n",
            "14200 entries read...\n",
            "14400 entries read...\n",
            "14600 entries read...\n",
            "14800 entries read...\n",
            "15000 entries read...\n",
            "15200 entries read...\n",
            "15400 entries read...\n",
            "15600 entries read...\n",
            "15800 entries read...\n",
            "16000 entries read...\n",
            "16200 entries read...\n",
            "16400 entries read...\n",
            "16600 entries read...\n",
            "16800 entries read...\n",
            "17000 entries read...\n",
            "17200 entries read...\n",
            "17400 entries read...\n",
            "17600 entries read...\n",
            "17800 entries read...\n",
            "18000 entries read...\n",
            "18200 entries read...\n",
            "18400 entries read...\n",
            "18600 entries read...\n",
            "18800 entries read...\n",
            "19000 entries read...\n",
            "19200 entries read...\n",
            "19400 entries read...\n",
            "19600 entries read...\n",
            "19800 entries read...\n",
            "20000 entries read...\n",
            "20200 entries read...\n",
            "20400 entries read...\n",
            "20600 entries read...\n",
            "20800 entries read...\n",
            "21000 entries read...\n",
            "21200 entries read...\n",
            "21400 entries read...\n",
            "21600 entries read...\n",
            "21800 entries read...\n",
            "22000 entries read...\n",
            "22200 entries read...\n",
            "22400 entries read...\n",
            "22600 entries read...\n",
            "22800 entries read...\n",
            "23000 entries read...\n",
            "23200 entries read...\n",
            "23400 entries read...\n",
            "23600 entries read...\n",
            "23800 entries read...\n",
            "24000 entries read...\n",
            "24200 entries read...\n",
            "24400 entries read...\n",
            "passed here\n",
            "Reading data...\n",
            "200 entries read...\n",
            "400 entries read...\n",
            "600 entries read...\n",
            "800 entries read...\n",
            "1000 entries read...\n",
            "1200 entries read...\n",
            "1400 entries read...\n",
            "1600 entries read...\n",
            "1800 entries read...\n",
            "2000 entries read...\n",
            "2200 entries read...\n",
            "2400 entries read...\n",
            "2600 entries read...\n",
            "2800 entries read...\n",
            "3000 entries read...\n",
            "3200 entries read...\n",
            "3400 entries read...\n",
            "3600 entries read...\n",
            "3800 entries read...\n",
            "4000 entries read...\n",
            "4200 entries read...\n",
            "4400 entries read...\n",
            "4600 entries read...\n",
            "4800 entries read...\n",
            "5000 entries read...\n",
            "5200 entries read...\n",
            "5400 entries read...\n",
            "5600 entries read...\n",
            "Finished reading data!\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"arritmia_cardiaca.ipynb\n",
        "\n",
        "Automatically generated by Colaboratory.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1kIePtG_yixpzcbjVl5mY7nwQkAytTe_V\n",
        "\"\"\"\n",
        "\n",
        "# Credits to https://towardsdatascience.com/multi-task-learning-for-computer-vision-classification-with-keras-36c52e6243d2\n",
        "\n",
        "# Imports\n",
        "print(\"start imports\")\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import sklearn\n",
        "import time\n",
        "import os\n",
        "import csv\n",
        "import random\n",
        "from tensorflow import keras\n",
        "print(\"end imports\")\n",
        "\n",
        "\n",
        "# Custom stuff from https://github.com/GATECH-EIC/TinyML-Contest-Solution/blob/master/Training/select_model.py\n",
        "#from helpers.cosine_annealing import CosineAnnealingScheduler\n",
        "from swa.tfkeras import SWA\n",
        "\n",
        "# explicit function to normalize array\n",
        "def normalize(arr, t_min=0, t_max=1):\n",
        "    norm_arr = []\n",
        "    diff = t_max - t_min\n",
        "    diff_arr = max(arr) - min(arr)\n",
        "    for i in arr:\n",
        "        temp = (((i - min(arr))*diff)/diff_arr) + t_min\n",
        "        norm_arr.append(temp)\n",
        "    return norm_arr\n",
        "\n",
        "\"\"\" Functions for loading data \"\"\"\n",
        "def txt_to_numpy(filename, row=1250):\n",
        "    file = open(filename)\n",
        "    lines = file.readlines()\n",
        "    datamat = np.arange(row, dtype=np.float)\n",
        "    row_count = 0\n",
        "    for line in lines:\n",
        "        line = line.strip().split(' ')\n",
        "        datamat[row_count] = line[0]\n",
        "        row_count += 1\n",
        "\n",
        "    return datamat\n",
        "\n",
        "\"\"\"\n",
        "AFb,Atrial Fibrillation\n",
        "AFt,Atrial Flutter\n",
        "SR,Sinus Rhythm\n",
        "SVT,Supraventricular Tachycardia\n",
        "VFb,Ventricular Fibrillation\n",
        "VFt,Ventricular Flutter\n",
        "VPD,Ventricular Premature Depolarizations\n",
        "VT,Ventricular Tachycardia\n",
        "\"\"\"\n",
        "def txt_to_disease_type(txt):\n",
        "    txt = txt.upper()\n",
        "    if txt == \"AFB\":\n",
        "      return [1, 0, 0, 0, 0, 0, 0, 0]\n",
        "    if txt == \"AFT\":\n",
        "      return [0, 1, 0, 0, 0, 0, 0, 0]\n",
        "    if txt == \"SR\":\n",
        "      return [0, 0, 1, 0, 0, 0, 0, 0]\n",
        "    if txt == \"SVT\":\n",
        "      return [0, 0, 0, 1, 0, 0, 0, 0]\n",
        "    if txt == \"VFB\":\n",
        "      return [0, 0, 0, 0, 1, 0, 0, 0]\n",
        "    if txt == \"VFT\":\n",
        "      return [0, 0, 0, 0, 0, 1, 0, 0]\n",
        "    if txt == \"VPD\":\n",
        "      return [0, 0, 0, 0, 0, 0, 1, 0]\n",
        "    if txt == \"VT\":\n",
        "      return [0, 0, 0, 0, 0, 0, 0, 1]\n",
        "    raise \"A string was not recognized as a valid input on txt_to_disease_type func!\"\n",
        "\n",
        "\"\"\"\n",
        "def read_data(csv_path, imgs_folder=\"./tinyml_contest_data_training\"):\n",
        "    x, y1, y2 = [], [], []\n",
        "    with open(csv_path, \"r\") as csv_file:\n",
        "      reader = csv.reader(csv_file)\n",
        "      next(reader)\n",
        "      for item in reader: # item[0] = label; item[1] = filename\n",
        "        x.append(txt_to_numpy(os.path.join(imgs_folder, item[1]), 1250))\n",
        "        y1.append([1, 0] if int(item[0]) == 0 else [0, 1]) # Label for life threat\n",
        "        y2.append(txt_to_disease_type(item[1].split(\"-\")[1]))\n",
        "    x, y1, y2 = sklearn.utils.shuffle(x, y1, y2)\n",
        "    return np.array(x), np.array(y1), np.array(y2)\n",
        "\"\"\"\n",
        "def read_data(csv_path, imgs_folder=\"./tinyml_contest_data_training\", augmentation=False, flip_peak=True, flip_time=True, add_noise=True): #, flip_peak=False, flip_time=False, add_noise=False):\n",
        "    x, y1, y2, y3, y4, y5 = [], [], [], [], [], []\n",
        "    print(\"passed here\")\n",
        "    with open(csv_path, \"r\") as csv_file:\n",
        "      reader = csv.reader(csv_file)\n",
        "      next(reader)\n",
        "      print(\"Reading data...\")\n",
        "      counter = 0\n",
        "      for item in reader: # item[0] = label; item[1] = filename\n",
        "        x.append(txt_to_numpy(os.path.join(imgs_folder, item[1]), 1250))\n",
        "        y1.append([1, 0] if int(item[0]) == 0 else [0, 1]) # Label for life threat\n",
        "        y2.append(txt_to_disease_type(item[1].split(\"-\")[1]))\n",
        "        counter +=1\n",
        "        if counter % 200 == 0:\n",
        "          print(f\"{counter} entries read...\")\n",
        "    x, y1, y2 = sklearn.utils.shuffle(x, y1, y2)\n",
        "    x = np.array(x)\n",
        "\n",
        "    # y3 creation\n",
        "    for sample in x:\n",
        "       y3.append(np.mean(sample))\n",
        "       stdeviation = np.std(sample)\n",
        "       y4.append(stdeviation)\n",
        "       y5.append(stdeviation ** 2)\n",
        "\n",
        "    if augmentation:\n",
        "      #x = x + np.random.normal(0, random.random()*np.amax(x)*0.05, (len(x), 1))\n",
        "\n",
        "      for i in range(len(x)):\n",
        "        flip_p = random.random()\n",
        "        flip_t = random.random()\n",
        "        if flip_p < 0.5 and flip_peak:\n",
        "          x[i] = -x[i]\n",
        "        if flip_t < 0.5 and flip_time:\n",
        "          x[i] = np.flip(x[i])\n",
        "        if add_noise:\n",
        "          print(f\"i={i}\")\n",
        "          max_peak = x[i].max() * 0.05\n",
        "          factor = random.random()\n",
        "        # factor = 1\n",
        "          noise = np.random.normal(0, factor * max_peak, x[0].shape)\n",
        "          x[i] = x[i] + noise\n",
        "    return x, np.array(y1), np.array(y2), np.array(y3), np.array(y4), np.array(y5)\n",
        "\n",
        "\n",
        "\"\"\" Loading data \"\"\"\n",
        "x_train, y_train_1, y_train_2, y_train_3, y_train_4, y_train_5 = read_data(\"./data_indices/train_indice.csv\", augmentation=False) #augmentation=True, flip_peak=True, flip_time=True, add_noise=True)\n",
        "x_test, y_test_1, y_test_2, y_test_3, y_test_4, y_test_5 = read_data(\"./data_indices/test_indice.csv\")\n",
        "print(\"Finished reading data!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train, _, y_train_1, _, = train_test_split(x_train, y_train_1, train_size=0.4)"
      ],
      "metadata": {
        "id": "NPZKrKibMn3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7QqsS-_9DI5X",
        "outputId": "31d70617-f859-414f-fc2b-3e6681b2a0e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training on batch of models for gamma values  [[1, 0, 0, 0, 0]] \n",
            "\n",
            "\n",
            "Training model for gamma equal to  [1, 0, 0, 0, 0]\n",
            "Epoch 1/30\n",
            "769/769 [==============================] - 14s 12ms/step - loss: 0.5012 - accuracy: 0.7758 - auc_1: 0.8523 - precision_1: 0.7758 - recall_1: 0.7758\n",
            "Epoch 2/30\n",
            "769/769 [==============================] - 8s 10ms/step - loss: 0.3351 - accuracy: 0.8619 - auc_1: 0.9327 - precision_1: 0.8619 - recall_1: 0.8619\n",
            "Epoch 3/30\n",
            "769/769 [==============================] - 9s 11ms/step - loss: 0.2645 - accuracy: 0.8918 - auc_1: 0.9567 - precision_1: 0.8918 - recall_1: 0.8918\n",
            "Epoch 4/30\n",
            "769/769 [==============================] - 8s 10ms/step - loss: 0.2283 - accuracy: 0.9105 - auc_1: 0.9665 - precision_1: 0.9105 - recall_1: 0.9105\n",
            "Epoch 5/30\n",
            "769/769 [==============================] - 8s 11ms/step - loss: 0.1944 - accuracy: 0.9238 - auc_1: 0.9748 - precision_1: 0.9238 - recall_1: 0.9238\n",
            "Epoch 6/30\n",
            "769/769 [==============================] - 8s 11ms/step - loss: 0.1793 - accuracy: 0.9308 - auc_1: 0.9774 - precision_1: 0.9308 - recall_1: 0.9308\n",
            "Epoch 7/30\n",
            "769/769 [==============================] - 8s 10ms/step - loss: 0.1554 - accuracy: 0.9393 - auc_1: 0.9824 - precision_1: 0.9393 - recall_1: 0.9393\n",
            "Epoch 8/30\n",
            "769/769 [==============================] - 9s 12ms/step - loss: 0.1426 - accuracy: 0.9427 - auc_1: 0.9846 - precision_1: 0.9427 - recall_1: 0.9427\n",
            "Epoch 9/30\n",
            "769/769 [==============================] - 10s 13ms/step - loss: 0.1290 - accuracy: 0.9520 - auc_1: 0.9860 - precision_1: 0.9520 - recall_1: 0.9520\n",
            "Epoch 10/30\n",
            "769/769 [==============================] - 10s 12ms/step - loss: 0.1188 - accuracy: 0.9542 - auc_1: 0.9883 - precision_1: 0.9542 - recall_1: 0.9542\n",
            "Epoch 11/30\n",
            "769/769 [==============================] - 8s 10ms/step - loss: 0.1110 - accuracy: 0.9573 - auc_1: 0.9895 - precision_1: 0.9573 - recall_1: 0.9573\n",
            "Epoch 12/30\n",
            "769/769 [==============================] - 9s 12ms/step - loss: 0.1078 - accuracy: 0.9600 - auc_1: 0.9893 - precision_1: 0.9600 - recall_1: 0.9600\n",
            "Epoch 13/30\n",
            "769/769 [==============================] - 9s 11ms/step - loss: 0.1054 - accuracy: 0.9603 - auc_1: 0.9897 - precision_1: 0.9603 - recall_1: 0.9603\n",
            "Epoch 14/30\n",
            "769/769 [==============================] - 8s 10ms/step - loss: 0.0894 - accuracy: 0.9661 - auc_1: 0.9922 - precision_1: 0.9661 - recall_1: 0.9661\n",
            "Epoch 15/30\n",
            "769/769 [==============================] - 8s 11ms/step - loss: 0.0902 - accuracy: 0.9664 - auc_1: 0.9922 - precision_1: 0.9664 - recall_1: 0.9664\n",
            "Epoch 16/30\n",
            "769/769 [==============================] - 8s 10ms/step - loss: 0.0854 - accuracy: 0.9679 - auc_1: 0.9922 - precision_1: 0.9679 - recall_1: 0.9679\n",
            "Epoch 17/30\n",
            "769/769 [==============================] - 8s 10ms/step - loss: 0.0775 - accuracy: 0.9719 - auc_1: 0.9932 - precision_1: 0.9719 - recall_1: 0.9719\n",
            "Epoch 18/30\n",
            "769/769 [==============================] - 8s 11ms/step - loss: 0.0782 - accuracy: 0.9708 - auc_1: 0.9930 - precision_1: 0.9708 - recall_1: 0.9708\n",
            "Epoch 19/30\n",
            "769/769 [==============================] - 8s 10ms/step - loss: 0.0725 - accuracy: 0.9730 - auc_1: 0.9939 - precision_1: 0.9730 - recall_1: 0.9730\n",
            "Epoch 20/30\n",
            "769/769 [==============================] - 8s 11ms/step - loss: 0.0690 - accuracy: 0.9759 - auc_1: 0.9940 - precision_1: 0.9759 - recall_1: 0.9759\n",
            "Epoch 21/30\n",
            "769/769 [==============================] - 8s 11ms/step - loss: 0.0578 - accuracy: 0.9793 - auc_1: 0.9961 - precision_1: 0.9793 - recall_1: 0.9793\n",
            "Epoch 22/30\n",
            "769/769 [==============================] - 8s 10ms/step - loss: 0.0618 - accuracy: 0.9768 - auc_1: 0.9952 - precision_1: 0.9768 - recall_1: 0.9768\n",
            "Epoch 23/30\n",
            "769/769 [==============================] - 9s 12ms/step - loss: 0.0630 - accuracy: 0.9764 - auc_1: 0.9951 - precision_1: 0.9764 - recall_1: 0.9764\n",
            "Epoch 24/30\n",
            "769/769 [==============================] - 8s 11ms/step - loss: 0.0627 - accuracy: 0.9772 - auc_1: 0.9946 - precision_1: 0.9772 - recall_1: 0.9772\n",
            "Epoch 25/30\n",
            "769/769 [==============================] - 9s 11ms/step - loss: 0.0569 - accuracy: 0.9790 - auc_1: 0.9959 - precision_1: 0.9790 - recall_1: 0.9790\n",
            "Epoch 26/30\n",
            "769/769 [==============================] - 10s 13ms/step - loss: 0.0548 - accuracy: 0.9807 - auc_1: 0.9956 - precision_1: 0.9807 - recall_1: 0.9807\n",
            "Epoch 27/30\n",
            "769/769 [==============================] - 10s 12ms/step - loss: 0.0535 - accuracy: 0.9805 - auc_1: 0.9961 - precision_1: 0.9805 - recall_1: 0.9805\n",
            "Epoch 28/30\n",
            "769/769 [==============================] - 8s 10ms/step - loss: 0.0520 - accuracy: 0.9814 - auc_1: 0.9956 - precision_1: 0.9814 - recall_1: 0.9814\n",
            "Epoch 29/30\n",
            "769/769 [==============================] - 8s 11ms/step - loss: 0.0475 - accuracy: 0.9826 - auc_1: 0.9965 - precision_1: 0.9826 - recall_1: 0.9826\n",
            "Epoch 30/30\n",
            "769/769 [==============================] - 8s 11ms/step - loss: 0.0435 - accuracy: 0.9843 - auc_1: 0.9969 - precision_1: 0.9843 - recall_1: 0.9843\n",
            "Training time: 265.5915138721466\n",
            "\n",
            "176/176 [==============================] - 1s 4ms/step - loss: 0.3477 - accuracy: 0.9522 - auc_1: 0.9642 - precision_1: 0.9522 - recall_1: 0.9522\n",
            "loss: 0.34774184226989746\n",
            "accuracy: 0.9521777629852295\n",
            "auc_1: 0.9641637206077576\n",
            "precision_1: 0.9521777629852295\n",
            "recall_1: 0.9521777629852295\n"
          ]
        }
      ],
      "source": [
        "#@title\n",
        "## NO MULTITASKINGGGG!!!!!!!!\n",
        "## NO MULTITASKINGGGG!!!!!!!!\n",
        "## NO MULTITASKINGGGG!!!!!!!!\n",
        "## NO MULTITASKINGGGG!!!!!!!!\n",
        "## NO MULTITASKINGGGG!!!!!!!!## NO MULTITASKINGGGG!!!!!!!!\n",
        "## NO MULTITASKINGGGG!!!!!!!!\n",
        "## NO MULTITASKINGGGG!!!!!!!!## NO MULTITASKINGGGG!!!!!!!!\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\"\"\" Functions for creating, compiling and training model\"\"\"\n",
        "\n",
        "# Architecture:\n",
        "# Main branch\n",
        "# Branch1 => Decides wheter life threatening (VF or VT style) or not (SR and others)\n",
        "# Branch2 => Classify the input\n",
        "\n",
        "\n",
        "# labels,Rhythm\n",
        "# AFb,Atrial Fibrillation\n",
        "# AFt,Atrial Flutter\n",
        "# SR,Sinus Rhythm\n",
        "# SVT,Supraventricular Tachycardia\n",
        "# VFb,Ventricular Fibrillation\n",
        "# VFt,Ventricular Flutter\n",
        "# VPD,Ventricular Premature Depolarizations\n",
        "# VT,Ventricular Tachycardia\n",
        "\n",
        "\n",
        "def gen_model():\n",
        "    inputs = keras.layers.Input(shape=(1250, 1), name='input')\n",
        "\n",
        "    main_branch = keras.layers.Conv1D(filters=3, kernel_size=6, strides=2, activation=\"relu\") (inputs)\n",
        "    main_branch = keras.layers.BatchNormalization() (main_branch)\n",
        "    main_branch = keras.layers.Conv1D(filters=5, kernel_size=5, strides=2,  activation=\"relu\") (main_branch)\n",
        "    main_branch = keras.layers.BatchNormalization() (main_branch)\n",
        "    main_branch = keras.layers.Conv1D(filters=10, kernel_size=4, strides=2,  activation=\"relu\") (main_branch)\n",
        "    main_branch = keras.layers.BatchNormalization() (main_branch)\n",
        "    main_branch = keras.layers.Conv1D(filters=20, kernel_size=4, strides=2,  activation=\"relu\") (main_branch)\n",
        "    main_branch = keras.layers.BatchNormalization() (main_branch)\n",
        "    main_branch = keras.layers.Conv1D(filters=20, kernel_size=4, strides=2,  activation=\"relu\") (main_branch)\n",
        "    main_branch = keras.layers.BatchNormalization() (main_branch)\n",
        "    #main_branch = keras.layers.Conv1D(filters=32, kernel_size=1, strides=32, activation=\"relu\") (main_branch)\n",
        "    #main_branch = keras.layers.BatchNormalization() (main_branch)\n",
        "    main_branch = keras.layers.Flatten() (main_branch)\n",
        "    main_branch = keras.layers.Dropout(0.5) (main_branch)\n",
        "    #main_branch = keras.layers.Dense(512, activation=\"relu\") (main_branch)\n",
        "    #main_branch = keras.layers.Dropout(0.1) (main_branch)\n",
        "    #main_branch = keras.layers.Dense(512, activation=\"relu\") (main_branch)\n",
        "    #main_branch = keras.layers.Dropout(0.1) (main_branch)\n",
        "    main_branch = keras.layers.Dense(512, activation=\"relu\") (main_branch)\n",
        "    main_branch = keras.layers.BatchNormalization() (main_branch)\n",
        "\n",
        "    #branch1 = keras.layers.Dense(128, activation=\"relu\") (branch1)\n",
        "    #branch1 = keras.layers.BatchNormalization() (branch1)\n",
        "    #branch1 = keras.layers.Dense(128, activation=\"relu\") (branch1)\n",
        "    #branch1 = keras.layers.BatchNormalization() (branch1)\n",
        "    #branch1 = keras.layers.Dropout(0.2) (branch1)\n",
        "\n",
        "    # Decides wheter life threatening (VF or VT style) or not (SR and others)\n",
        "    branch1 = keras.layers.Dense(2, activation=\"softmax\", name='task_1_output') (main_branch)\n",
        "\n",
        "    #branch2 = keras.layers.Dense(128, activation=\"relu\") (main_branch)\n",
        "    #branch2 = keras.layers.BatchNormalization() (branch2)\n",
        "    #branch2 = keras.layers.Dense(128, activation=\"relu\") (branch2)\n",
        "    #branch2 = keras.layers.BatchNormalization() (branch2)\n",
        "    #branch2 = keras.layers.Dense(128, activation=\"relu\") (branch2)\n",
        "    #branch2 = keras.layers.BatchNormalization() (branch2)\n",
        "    #branch2 = keras.layers.Dropout(0.2) (branch2)\n",
        "\n",
        "    # Classify the input\n",
        "    #branch2 = keras.layers.Dense(8, activation=\"softmax\", name='task_2_output') (main_branch)\n",
        "#\n",
        "    ## mean\n",
        "    #branch3 = keras.layers.Dense(1, activation=\"relu\", name='task_3_output') (main_branch)\n",
        "#\n",
        "    ## std\n",
        "    #branch4 = keras.layers.Dense(1, activation=\"relu\", name='task_4_output') (main_branch)\n",
        "#\n",
        "    ## var\n",
        "    #branch5 = keras.layers.Dense(1, activation=\"relu\", name='task_5_output') (main_branch)\n",
        "\n",
        "    model = tf.keras.Model(inputs = inputs, outputs = branch1)#, branch2, branch3, branch4, branch5])\n",
        "\n",
        "    return model\n",
        "\n",
        "# loss_weight -> [0, 1]\n",
        "# the weight for the second task is calculated by '1 - loss_weight'\n",
        "def compile_model(model, loss_weight1, loss_weight2, loss_weight3, loss_weight4, loss_weight5):\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(0.0002),\n",
        "                  loss={'task_1_output': 'binary_crossentropy',\n",
        "                        #'task_2_output': 'categorical_crossentropy',\n",
        "                        #'task_3_output': 'mse',\n",
        "                        #'task_4_output': 'mse',\n",
        "                        #'task_5_output': 'mse'\n",
        "                        },\n",
        "                  loss_weights={'task_1_output': loss_weight1,\n",
        "                                #'task_2_output': loss_weight2,\n",
        "                                #'task_3_output': loss_weight3,\n",
        "                                #'task_4_output': loss_weight4,\n",
        "                                #'task_5_output': loss_weight5\n",
        "                                },\n",
        "                  metrics={'task_1_output' : [\"accuracy\", tf.keras.metrics.AUC(), tf.keras.metrics.Precision(), tf.keras.metrics.Recall()],\n",
        "                           #'task_2_output' : [\"accuracy\", tf.keras.metrics.Precision(), tf.keras.metrics.Recall()],\n",
        "                           #'task_3_output' : \"mse\",\n",
        "                           #'task_4_output' : \"mse\",\n",
        "                           #'task_5_output' : \"mse\"\n",
        "                           })\n",
        "    return model\n",
        "\n",
        "def fit_batch(gamma_values, epochs=30, batch_size=32, save_models=False, models_dir=\"./trained_models\", verbose=0):\n",
        "\n",
        "    history = list()\n",
        "    trained_models = list()\n",
        "    test_scores = list()\n",
        "\n",
        "    start_epoch = 10\n",
        "    swa = SWA(start_epoch=start_epoch,\n",
        "          lr_schedule='cyclic',\n",
        "          swa_lr=0.0001,\n",
        "          swa_lr2=0.0005,\n",
        "          swa_freq=5,\n",
        "          batch_size=batch_size,\n",
        "          verbose=1)\n",
        "\n",
        "    if save_models and not os.path.isdir(models_dir):\n",
        "      os.mkdir(models_dir)\n",
        "\n",
        "    print('Starting training on batch of models for gamma values ', gamma_values, '\\n\\n')\n",
        "\n",
        "    for i, gamma in enumerate(gamma_values):\n",
        "\n",
        "        print('Training model for gamma equal to ', gamma)\n",
        "        model = gen_model()\n",
        "        model = compile_model(model, gamma[0], gamma[1], gamma[2], gamma[3], gamma[4])\n",
        "        start = time.time()\n",
        "        model_history = model.fit({'input': x_train},\n",
        "                            {'task_1_output': y_train_1, #'task_2_output': y_train_2, 'task_3_output': y_train_3, 'task_4_output': y_train_4, 'task_5_output': y_train_5\n",
        "                             },\n",
        "                            epochs=epochs, batch_size=batch_size, verbose=verbose) # callbacks=[swa],\n",
        "        print(f'Training time: {time.time() - start}\\n')\n",
        "        history.append(model_history)\n",
        "        trained_models.append(model)\n",
        "        if save_models:\n",
        "          model.save(os.path.join(models_dir, f\"{i}th-model\"))\n",
        "\n",
        "        test_score = model.evaluate({'input': x_test}, {'task_1_output': y_test_1,\n",
        "                                                        #'task_2_output': y_test_2, 'task_3_output': y_test_3, 'task_4_output': y_test_4, 'task_5_output': y_test_5\n",
        "                                                        })\n",
        "        for metric, result in zip(model.metrics_names, test_score):\n",
        "          print(f\"{metric}: {result}\")\n",
        "\n",
        "        test_scores.append(test_score)\n",
        "\n",
        "    return history, trained_models, test_scores\n",
        "\n",
        "def plot_multitask_accuracies(gammas, training_history):\n",
        "\n",
        "    counter = 0\n",
        "\n",
        "    for history in training_history:\n",
        "\n",
        "        print(f'\\nPlotting Accuracy/Precision/Recall vs Epochs for value of gamma number {gammas[counter]}\\n')\n",
        "        plt.plot(range(len(history.history['task_1_output_accuracy'])), history.history['task_1_output_accuracy'], c='r', label='Task 1')\n",
        "        plt.plot(range(len(history.history['task_2_output_accuracy'])), history.history['task_2_output_accuracy'], c='b', label='Task 2')\n",
        "        plt.plot(range(len(history.history['task_1_output_precision'])), history.history['task_1_output_precision'], c='r', label='Task 1')\n",
        "        plt.plot(range(len(history.history['task_2_output_precision'])), history.history['task_2_output_precision'], c='b', label='Task 2')\n",
        "        plt.plot(range(len(history.history['task_1_output_recall'])), history.history['task_1_output_recall'], c='r', label='Task 1')\n",
        "        plt.plot(range(len(history.history['task_2_output_recall'])), history.history['task_2_output_recall'], c='b', label='Task 2')\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "        counter += 1\n",
        "\n",
        "\"\"\"Actually training the model\"\"\"\n",
        "#gamma_values = [ [0.2, 0.2, 0.2, 0.2, 0.2],\n",
        "#                [0.8, 0.1, 0.0333, 0.0333, 0.0333],\n",
        "#                [0.5, 0.125, 0.125, 0.125, 0.125],\n",
        "#                [0.5, 0.2, 0.1, 0.1, 0.1],\n",
        "#                [0.6, 0.3, 0.033, 0.033, 0.033]]\n",
        "\n",
        "gamma_values = [[1, 0, 0, 0, 0]]\n",
        "\n",
        "#for _ in range(20):\n",
        "\n",
        "history, trained_models, _ = fit_batch(gamma_values, epochs=30, save_models=True, models_dir=\"./trained_models_sixth_augmentation\", verbose=1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(y_train_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1fZM0SXNX9u",
        "outputId": "624a4c87-3f56-4691-afc7-04922b26eb15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "24588"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(y_train_1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "chzyQBwuNeHF",
        "outputId": "ee23c931-29ca-4ef4-d3a3-28dda0b44b50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9835"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(x_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OPP5E34BNgWJ",
        "outputId": "699bf587-6035-482b-a8f8-badd0d69e1f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5625"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "db-O5eI23n5x"
      },
      "source": [
        "NOT T TEST DOWN HERE!!!!!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5QC0bCGsIq9",
        "outputId": "6ce45b57-6d00-4669-b55e-7557a271f723"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting heartpy\n",
            "  Downloading heartpy-1.2.7-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from heartpy) (1.22.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from heartpy) (1.10.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from heartpy) (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->heartpy) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->heartpy) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->heartpy) (4.41.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->heartpy) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->heartpy) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->heartpy) (8.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->heartpy) (3.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->heartpy) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->heartpy) (1.16.0)\n",
            "Installing collected packages: heartpy\n",
            "Successfully installed heartpy-1.2.7\n"
          ]
        }
      ],
      "source": [
        "!pip install heartpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5c5trwS4q1BN",
        "outputId": "161f9231-ed84-4f70-f5f2-8aa75d07ca80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "start imports\n",
            "end imports\n",
            "passed here\n",
            "Reading data...\n",
            "200 entries read...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-28-9e22cc1e749d>:40: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  datamat = np.arange(row, dtype=np.float)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "400 entries read...\n",
            "600 entries read...\n",
            "800 entries read...\n",
            "1000 entries read...\n",
            "1200 entries read...\n",
            "1400 entries read...\n",
            "1600 entries read...\n",
            "1800 entries read...\n",
            "2000 entries read...\n",
            "2200 entries read...\n",
            "2400 entries read...\n",
            "2600 entries read...\n",
            "2800 entries read...\n",
            "3000 entries read...\n",
            "3200 entries read...\n",
            "3400 entries read...\n",
            "3600 entries read...\n",
            "3800 entries read...\n",
            "4000 entries read...\n",
            "4200 entries read...\n",
            "4400 entries read...\n",
            "4600 entries read...\n",
            "4800 entries read...\n",
            "5000 entries read...\n",
            "5200 entries read...\n",
            "5400 entries read...\n",
            "5600 entries read...\n",
            "5800 entries read...\n",
            "6000 entries read...\n",
            "6200 entries read...\n",
            "6400 entries read...\n",
            "6600 entries read...\n",
            "6800 entries read...\n",
            "7000 entries read...\n",
            "7200 entries read...\n",
            "7400 entries read...\n",
            "7600 entries read...\n",
            "7800 entries read...\n",
            "8000 entries read...\n",
            "8200 entries read...\n",
            "8400 entries read...\n",
            "8600 entries read...\n",
            "8800 entries read...\n",
            "9000 entries read...\n",
            "9200 entries read...\n",
            "9400 entries read...\n",
            "9600 entries read...\n",
            "9800 entries read...\n",
            "10000 entries read...\n",
            "10200 entries read...\n",
            "10400 entries read...\n",
            "10600 entries read...\n",
            "10800 entries read...\n",
            "11000 entries read...\n",
            "11200 entries read...\n",
            "11400 entries read...\n",
            "11600 entries read...\n",
            "11800 entries read...\n",
            "12000 entries read...\n",
            "12200 entries read...\n",
            "12400 entries read...\n",
            "12600 entries read...\n",
            "12800 entries read...\n",
            "13000 entries read...\n",
            "13200 entries read...\n",
            "13400 entries read...\n",
            "13600 entries read...\n",
            "13800 entries read...\n",
            "14000 entries read...\n",
            "14200 entries read...\n",
            "14400 entries read...\n",
            "14600 entries read...\n",
            "14800 entries read...\n",
            "15000 entries read...\n",
            "15200 entries read...\n",
            "15400 entries read...\n",
            "15600 entries read...\n",
            "15800 entries read...\n",
            "16000 entries read...\n",
            "16200 entries read...\n",
            "16400 entries read...\n",
            "16600 entries read...\n",
            "16800 entries read...\n",
            "17000 entries read...\n",
            "17200 entries read...\n",
            "17400 entries read...\n",
            "17600 entries read...\n",
            "17800 entries read...\n",
            "18000 entries read...\n",
            "18200 entries read...\n",
            "18400 entries read...\n",
            "18600 entries read...\n",
            "18800 entries read...\n",
            "19000 entries read...\n",
            "19200 entries read...\n",
            "19400 entries read...\n",
            "19600 entries read...\n",
            "19800 entries read...\n",
            "20000 entries read...\n",
            "20200 entries read...\n",
            "20400 entries read...\n",
            "20600 entries read...\n",
            "20800 entries read...\n",
            "21000 entries read...\n",
            "21200 entries read...\n",
            "21400 entries read...\n",
            "21600 entries read...\n",
            "21800 entries read...\n",
            "22000 entries read...\n",
            "22200 entries read...\n",
            "22400 entries read...\n",
            "22600 entries read...\n",
            "22800 entries read...\n",
            "23000 entries read...\n",
            "23200 entries read...\n",
            "23400 entries read...\n",
            "23600 entries read...\n",
            "23800 entries read...\n",
            "24000 entries read...\n",
            "24200 entries read...\n",
            "24400 entries read...\n",
            "passei\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numpy/ma/core.py:5246: RuntimeWarning: Mean of empty slice.\n",
            "  result = super().mean(axis=axis, dtype=dtype, **kwargs)[()]\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3757: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
            "  return _methods._var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
            "/usr/local/lib/python3.10/dist-packages/scipy/interpolate/_fitpack2.py:298: UserWarning: \n",
            "The maximal number of iterations maxit (set to 20 by the program)\n",
            "allowed for finding a smoothing spline with fp=s has been reached: s\n",
            "too small.\n",
            "There is an approximation returned but the corresponding weighted sum\n",
            "of squared residuals does not satisfy the condition abs(fp-s)/s < tol.\n",
            "  warnings.warn(message)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:264: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
            "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "passed here\n",
            "Reading data...\n",
            "200 entries read...\n",
            "400 entries read...\n",
            "600 entries read...\n",
            "800 entries read...\n",
            "1000 entries read...\n",
            "1200 entries read...\n",
            "1400 entries read...\n",
            "1600 entries read...\n",
            "1800 entries read...\n",
            "2000 entries read...\n",
            "2200 entries read...\n",
            "2400 entries read...\n",
            "2600 entries read...\n",
            "2800 entries read...\n",
            "3000 entries read...\n",
            "3200 entries read...\n",
            "3400 entries read...\n",
            "3600 entries read...\n",
            "3800 entries read...\n",
            "4000 entries read...\n",
            "4200 entries read...\n",
            "4400 entries read...\n",
            "4600 entries read...\n",
            "4800 entries read...\n",
            "5000 entries read...\n",
            "5200 entries read...\n",
            "5400 entries read...\n",
            "5600 entries read...\n",
            "passei\n",
            "Finished reading data!\n"
          ]
        }
      ],
      "source": [
        "# Normalizing data\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"arritmia_cardiaca.ipynb\n",
        "\n",
        "Automatically generated by Colaboratory.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1kIePtG_yixpzcbjVl5mY7nwQkAytTe_V\n",
        "\"\"\"\n",
        "\n",
        "# Credits to https://towardsdatascience.com/multi-task-learning-for-computer-vision-classification-with-keras-36c52e6243d2\n",
        "\n",
        "# Imports\n",
        "print(\"start imports\")\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import sklearn\n",
        "import time\n",
        "import os\n",
        "import csv\n",
        "import random\n",
        "import heartpy as hp\n",
        "from sklearn import preprocessing\n",
        "from tensorflow import keras\n",
        "print(\"end imports\")\n",
        "\n",
        "\n",
        "# Custom stuff from https://github.com/GATECH-EIC/TinyML-Contest-Solution/blob/master/Training/select_model.py\n",
        "#from helpers.cosine_annealing import CosineAnnealingScheduler\n",
        "from swa.tfkeras import SWA\n",
        "\n",
        "def norm(x):\n",
        "  return (x-np.min(x))/(np.max(x)-np.min(x))\n",
        "\n",
        "\"\"\" Functions for loading data \"\"\"\n",
        "def txt_to_numpy(filename, row=1250):\n",
        "    file = open(filename)\n",
        "    lines = file.readlines()\n",
        "    datamat = np.arange(row, dtype=np.float)\n",
        "    row_count = 0\n",
        "    for line in lines:\n",
        "        line = line.strip().split(' ')\n",
        "        datamat[row_count] = line[0]\n",
        "        row_count += 1\n",
        "\n",
        "    return datamat\n",
        "\n",
        "\"\"\"\n",
        "AFb,Atrial Fibrillation\n",
        "AFt,Atrial Flutter\n",
        "SR,Sinus Rhythm\n",
        "SVT,Supraventricular Tachycardia\n",
        "VFb,Ventricular Fibrillation\n",
        "VFt,Ventricular Flutter\n",
        "VPD,Ventricular Premature Depolarizations\n",
        "VT,Ventricular Tachycardia\n",
        "\"\"\"\n",
        "def txt_to_disease_type(txt):\n",
        "    txt = txt.upper()\n",
        "    if txt == \"AFB\":\n",
        "      return [1, 0, 0, 0, 0, 0, 0, 0]\n",
        "    if txt == \"AFT\":\n",
        "      return [0, 1, 0, 0, 0, 0, 0, 0]\n",
        "    if txt == \"SR\":\n",
        "      return [0, 0, 1, 0, 0, 0, 0, 0]\n",
        "    if txt == \"SVT\":\n",
        "      return [0, 0, 0, 1, 0, 0, 0, 0]\n",
        "    if txt == \"VFB\":\n",
        "      return [0, 0, 0, 0, 1, 0, 0, 0]\n",
        "    if txt == \"VFT\":\n",
        "      return [0, 0, 0, 0, 0, 1, 0, 0]\n",
        "    if txt == \"VPD\":\n",
        "      return [0, 0, 0, 0, 0, 0, 1, 0]\n",
        "    if txt == \"VT\":\n",
        "      return [0, 0, 0, 0, 0, 0, 0, 1]\n",
        "    raise \"A string was not recognized as a valid input on txt_to_disease_type func!\"\n",
        "\n",
        "\"\"\"\n",
        "def read_data(csv_path, imgs_folder=\"./tinyml_contest_data_training\"):\n",
        "    x, y1, y2 = [], [], []\n",
        "    with open(csv_path, \"r\") as csv_file:\n",
        "      reader = csv.reader(csv_file)\n",
        "      next(reader)\n",
        "      for item in reader: # item[0] = label; item[1] = filename\n",
        "        x.append(txt_to_numpy(os.path.join(imgs_folder, item[1]), 1250))\n",
        "        y1.append([1, 0] if int(item[0]) == 0 else [0, 1]) # Label for life threat\n",
        "        y2.append(txt_to_disease_type(item[1].split(\"-\")[1]))\n",
        "    x, y1, y2 = sklearn.utils.shuffle(x, y1, y2)\n",
        "    return np.array(x), np.array(y1), np.array(y2)\n",
        "\"\"\"\n",
        "def read_data(csv_path, imgs_folder=\"./tinyml_contest_data_training\", augmentation=False, flip_peak=True, flip_time=True, add_noise=True): #, flip_peak=False, flip_time=False, add_noise=False):\n",
        "    x, y1, y2, y3, y4, y5 = [], [], [], [], [], []\n",
        "    print(\"passed here\")\n",
        "    with open(csv_path, \"r\") as csv_file:\n",
        "      reader = csv.reader(csv_file)\n",
        "      next(reader)\n",
        "      print(\"Reading data...\")\n",
        "      counter = 0\n",
        "      for item in reader: # item[0] = label; item[1] = filename\n",
        "        x.append(norm(np.array(txt_to_numpy(os.path.join(imgs_folder, item[1]), 1250))))\n",
        "        y1.append([1, 0] if int(item[0]) == 0 else [0, 1]) # Label for life threat\n",
        "        y2.append(txt_to_disease_type(item[1].split(\"-\")[1]))\n",
        "        counter +=1\n",
        "        if counter % 200 == 0:\n",
        "          print(f\"{counter} entries read...\")\n",
        "    #x, y1, y2 = sklearn.utils.shuffle(x, y1, y2)\n",
        "    x, y1 = sklearn.utils.shuffle(x, y1)\n",
        "    print(\"passei\")\n",
        "    x = np.array(x)\n",
        "    y3 = []\n",
        "\n",
        "\n",
        "    for sample in x:\n",
        "      _, w = hp.process(sample, 250, bpmmin=0, bpmmax=300)\n",
        "      y3.append(w['breathingrate'])\n",
        "\n",
        "\n",
        "\n",
        "    ## y3 creation\n",
        "    #for sample in x:\n",
        "    #   y3.append(np.mean(sample))\n",
        "    #   stdeviation = np.std(sample)\n",
        "    #   y4.append(stdeviation)\n",
        "    #   y5.append(stdeviation ** 2)\n",
        "\n",
        "    if augmentation:\n",
        "      #x = x + np.random.normal(0, random.random()*np.amax(x)*0.05, (len(x), 1))\n",
        "\n",
        "      for i in range(len(x)):\n",
        "        flip_p = random.random()\n",
        "        flip_t = random.random()\n",
        "        if flip_p < 0.5 and flip_peak:\n",
        "          x[i] = -x[i]\n",
        "        if flip_t < 0.5 and flip_time:\n",
        "          x[i] = np.flip(x[i])\n",
        "        if add_noise:\n",
        "          print(f\"i={i}\")\n",
        "          max_peak = x[i].max() * 0.05\n",
        "          factor = random.random()\n",
        "        # factor = 1\n",
        "          noise = np.random.normal(0, factor * max_peak, x[0].shape)\n",
        "          x[i] = x[i] + noise\n",
        "    return x, np.array(y1), np.array(y2), np.array(y3), np.array(y4), np.array(y5)\n",
        "\n",
        "\n",
        "\"\"\" Loading data \"\"\"\n",
        "x_train, y_train_1, y_train_2, y_train_3, y_train_4, y_train_5 = read_data(\"./data_indices/train_indice.csv\", augmentation=False) #augmentation=True, flip_peak=True, flip_time=True, add_noise=True)\n",
        "x_test, y_test_1, y_test_2, y_test_3, y_test_4, y_test_5 = read_data(\"./data_indices/test_indice.csv\")\n",
        "print(\"Finished reading data!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6U7Kq8SU6b_N",
        "outputId": "09860687-fd8e-4b8e-f137-7d44e158432a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.4456328  0.         0.31289111 ... 3.08641975 0.26315789 0.30978934]\n"
          ]
        }
      ],
      "source": [
        "print(y_train_3)\n",
        "where_are_NaNs = np.isnan(y_train_3)\n",
        "y_train_3[where_are_NaNs] = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fKO8gvLz3lOp",
        "outputId": "f09d9e43-d41c-4d5c-a34c-d1b64f9e4028"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting training on batch of models for gamma values  [[0.7, 0.2, 0.1, 0, 0]] \n",
            "\n",
            "\n",
            "Training model for gamma equal to  [0.7, 0.2, 0.1, 0, 0]\n",
            "Training time: 205.81774020195007\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "176/176 [==============================] - 2s 5ms/step - loss: nan - task_1_output_loss: 0.1691 - task_2_output_loss: 1.3755 - task_3_output_loss: nan - task_1_output_accuracy: 0.9586 - task_1_output_precision_12: 0.9586 - task_1_output_recall_12: 0.9586 - task_2_output_accuracy: 0.4146 - task_2_output_precision_13: 0.4184 - task_2_output_recall_13: 0.1108 - task_3_output_mse: nan\n",
            "loss: nan\n",
            "task_1_output_loss: 0.16906456649303436\n",
            "task_2_output_loss: 1.3755264282226562\n",
            "task_3_output_loss: nan\n",
            "task_1_output_accuracy: 0.9585777521133423\n",
            "task_1_output_precision_12: 0.9585777521133423\n",
            "task_1_output_recall_12: 0.9585777521133423\n",
            "task_2_output_accuracy: 0.41457778215408325\n",
            "task_2_output_precision_13: 0.4184015989303589\n",
            "task_2_output_recall_13: 0.110755555331707\n",
            "task_3_output_mse: nan\n",
            "Starting training on batch of models for gamma values  [[0.7, 0.2, 0.1, 0, 0]] \n",
            "\n",
            "\n",
            "Training model for gamma equal to  [0.7, 0.2, 0.1, 0, 0]\n",
            "Training time: 206.77389812469482\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "176/176 [==============================] - 1s 5ms/step - loss: nan - task_1_output_loss: 0.1740 - task_2_output_loss: 1.3772 - task_3_output_loss: nan - task_1_output_accuracy: 0.9547 - task_1_output_precision_14: 0.9547 - task_1_output_recall_14: 0.9547 - task_2_output_accuracy: 0.4133 - task_2_output_precision_15: 0.4001 - task_2_output_recall_15: 0.1214 - task_3_output_mse: nan\n",
            "loss: nan\n",
            "task_1_output_loss: 0.1739819496870041\n",
            "task_2_output_loss: 1.3772070407867432\n",
            "task_3_output_loss: nan\n",
            "task_1_output_accuracy: 0.9546666741371155\n",
            "task_1_output_precision_14: 0.9546666741371155\n",
            "task_1_output_recall_14: 0.9546666741371155\n",
            "task_2_output_accuracy: 0.41333332657814026\n",
            "task_2_output_precision_15: 0.4001171588897705\n",
            "task_2_output_recall_15: 0.12142222374677658\n",
            "task_3_output_mse: nan\n",
            "Starting training on batch of models for gamma values  [[0.7, 0.2, 0.1, 0, 0]] \n",
            "\n",
            "\n",
            "Training model for gamma equal to  [0.7, 0.2, 0.1, 0, 0]\n",
            "Training time: 179.913476228714\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "176/176 [==============================] - 2s 6ms/step - loss: nan - task_1_output_loss: 0.2069 - task_2_output_loss: 1.3719 - task_3_output_loss: nan - task_1_output_accuracy: 0.9545 - task_1_output_precision_16: 0.9545 - task_1_output_recall_16: 0.9545 - task_2_output_accuracy: 0.4252 - task_2_output_precision_17: 0.4327 - task_2_output_recall_17: 0.1298 - task_3_output_mse: nan\n",
            "loss: nan\n",
            "task_1_output_loss: 0.2069026231765747\n",
            "task_2_output_loss: 1.3719254732131958\n",
            "task_3_output_loss: nan\n",
            "task_1_output_accuracy: 0.9544888734817505\n",
            "task_1_output_precision_16: 0.9544888734817505\n",
            "task_1_output_recall_16: 0.9544888734817505\n",
            "task_2_output_accuracy: 0.42524445056915283\n",
            "task_2_output_precision_17: 0.432720810174942\n",
            "task_2_output_recall_17: 0.12977777421474457\n",
            "task_3_output_mse: nan\n",
            "Starting training on batch of models for gamma values  [[0.7, 0.2, 0.1, 0, 0]] \n",
            "\n",
            "\n",
            "Training model for gamma equal to  [0.7, 0.2, 0.1, 0, 0]\n",
            "Training time: 179.65370178222656\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "176/176 [==============================] - 2s 6ms/step - loss: nan - task_1_output_loss: 0.2024 - task_2_output_loss: 1.3831 - task_3_output_loss: nan - task_1_output_accuracy: 0.9547 - task_1_output_precision_18: 0.9547 - task_1_output_recall_18: 0.9547 - task_2_output_accuracy: 0.4098 - task_2_output_precision_19: 0.3990 - task_2_output_recall_19: 0.1120 - task_3_output_mse: nan\n",
            "loss: nan\n",
            "task_1_output_loss: 0.20244917273521423\n",
            "task_2_output_loss: 1.3831435441970825\n",
            "task_3_output_loss: nan\n",
            "task_1_output_accuracy: 0.9546666741371155\n",
            "task_1_output_precision_18: 0.9546666741371155\n",
            "task_1_output_recall_18: 0.9546666741371155\n",
            "task_2_output_accuracy: 0.40977779030799866\n",
            "task_2_output_precision_19: 0.39898669719696045\n",
            "task_2_output_recall_19: 0.1120000034570694\n",
            "task_3_output_mse: nan\n",
            "Starting training on batch of models for gamma values  [[0.7, 0.2, 0.1, 0, 0]] \n",
            "\n",
            "\n",
            "Training model for gamma equal to  [0.7, 0.2, 0.1, 0, 0]\n",
            "Training time: 206.30949330329895\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "176/176 [==============================] - 2s 6ms/step - loss: nan - task_1_output_loss: 0.1845 - task_2_output_loss: 1.3910 - task_3_output_loss: nan - task_1_output_accuracy: 0.9577 - task_1_output_precision_20: 0.9577 - task_1_output_recall_20: 0.9577 - task_2_output_accuracy: 0.4130 - task_2_output_precision_21: 0.4158 - task_2_output_recall_21: 0.1404 - task_3_output_mse: nan\n",
            "loss: nan\n",
            "task_1_output_loss: 0.18451490998268127\n",
            "task_2_output_loss: 1.3910338878631592\n",
            "task_3_output_loss: nan\n",
            "task_1_output_accuracy: 0.9576888680458069\n",
            "task_1_output_precision_20: 0.9576888680458069\n",
            "task_1_output_recall_20: 0.9576888680458069\n",
            "task_2_output_accuracy: 0.41297778487205505\n",
            "task_2_output_precision_21: 0.41578948497772217\n",
            "task_2_output_recall_21: 0.14044444262981415\n",
            "task_3_output_mse: nan\n",
            "Starting training on batch of models for gamma values  [[0.7, 0.2, 0.1, 0, 0]] \n",
            "\n",
            "\n",
            "Training model for gamma equal to  [0.7, 0.2, 0.1, 0, 0]\n",
            "Training time: 179.94331455230713\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "176/176 [==============================] - 2s 6ms/step - loss: nan - task_1_output_loss: 0.1801 - task_2_output_loss: 1.3919 - task_3_output_loss: nan - task_1_output_accuracy: 0.9483 - task_1_output_precision_22: 0.9483 - task_1_output_recall_22: 0.9483 - task_2_output_accuracy: 0.3874 - task_2_output_precision_23: 0.3623 - task_2_output_recall_23: 0.1239 - task_3_output_mse: nan\n",
            "loss: nan\n",
            "task_1_output_loss: 0.180060476064682\n",
            "task_2_output_loss: 1.3918795585632324\n",
            "task_3_output_loss: nan\n",
            "task_1_output_accuracy: 0.9482666850090027\n",
            "task_1_output_precision_22: 0.9482666850090027\n",
            "task_1_output_recall_22: 0.9482666850090027\n",
            "task_2_output_accuracy: 0.3873777687549591\n",
            "task_2_output_precision_23: 0.3622661232948303\n",
            "task_2_output_recall_23: 0.12391111254692078\n",
            "task_3_output_mse: nan\n",
            "Starting training on batch of models for gamma values  [[0.7, 0.2, 0.1, 0, 0]] \n",
            "\n",
            "\n",
            "Training model for gamma equal to  [0.7, 0.2, 0.1, 0, 0]\n",
            "Training time: 205.51400423049927\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "176/176 [==============================] - 2s 6ms/step - loss: nan - task_1_output_loss: 0.1856 - task_2_output_loss: 1.3833 - task_3_output_loss: nan - task_1_output_accuracy: 0.9552 - task_1_output_precision_24: 0.9552 - task_1_output_recall_24: 0.9552 - task_2_output_accuracy: 0.4119 - task_2_output_precision_25: 0.4016 - task_2_output_recall_25: 0.1371 - task_3_output_mse: nan\n",
            "loss: nan\n",
            "task_1_output_loss: 0.1856347620487213\n",
            "task_2_output_loss: 1.3833025693893433\n",
            "task_3_output_loss: nan\n",
            "task_1_output_accuracy: 0.9552000164985657\n",
            "task_1_output_precision_24: 0.9552000164985657\n",
            "task_1_output_recall_24: 0.9552000164985657\n",
            "task_2_output_accuracy: 0.41191110014915466\n",
            "task_2_output_precision_25: 0.40156251192092896\n",
            "task_2_output_recall_25: 0.13706666231155396\n",
            "task_3_output_mse: nan\n",
            "Starting training on batch of models for gamma values  [[0.7, 0.2, 0.1, 0, 0]] \n",
            "\n",
            "\n",
            "Training model for gamma equal to  [0.7, 0.2, 0.1, 0, 0]\n",
            "Training time: 206.06904220581055\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "176/176 [==============================] - 2s 6ms/step - loss: nan - task_1_output_loss: 8.2136 - task_2_output_loss: 28.7944 - task_3_output_loss: nan - task_1_output_accuracy: 0.4247 - task_1_output_precision_26: 0.4247 - task_1_output_recall_26: 0.4247 - task_2_output_accuracy: 0.4690 - task_2_output_precision_27: 0.4690 - task_2_output_recall_27: 0.4690 - task_3_output_mse: nan\n",
            "loss: nan\n",
            "task_1_output_loss: 8.213595390319824\n",
            "task_2_output_loss: 28.794361114501953\n",
            "task_3_output_loss: nan\n",
            "task_1_output_accuracy: 0.42471110820770264\n",
            "task_1_output_precision_26: 0.42471110820770264\n",
            "task_1_output_recall_26: 0.42471110820770264\n",
            "task_2_output_accuracy: 0.46897777915000916\n",
            "task_2_output_precision_27: 0.46897777915000916\n",
            "task_2_output_recall_27: 0.46897777915000916\n",
            "task_3_output_mse: nan\n",
            "Starting training on batch of models for gamma values  [[0.7, 0.2, 0.1, 0, 0]] \n",
            "\n",
            "\n",
            "Training model for gamma equal to  [0.7, 0.2, 0.1, 0, 0]\n",
            "Training time: 205.7227213382721\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "176/176 [==============================] - 2s 8ms/step - loss: nan - task_1_output_loss: 0.2030 - task_2_output_loss: 1.3718 - task_3_output_loss: nan - task_1_output_accuracy: 0.9556 - task_1_output_precision_28: 0.9556 - task_1_output_recall_28: 0.9556 - task_2_output_accuracy: 0.4276 - task_2_output_precision_29: 0.4334 - task_2_output_recall_29: 0.1268 - task_3_output_mse: nan\n",
            "loss: nan\n",
            "task_1_output_loss: 0.2029951810836792\n",
            "task_2_output_loss: 1.3718466758728027\n",
            "task_3_output_loss: nan\n",
            "task_1_output_accuracy: 0.9555555582046509\n",
            "task_1_output_precision_28: 0.9555555582046509\n",
            "task_1_output_recall_28: 0.9555555582046509\n",
            "task_2_output_accuracy: 0.42755556106567383\n",
            "task_2_output_precision_29: 0.4334346652030945\n",
            "task_2_output_recall_29: 0.12675555050373077\n",
            "task_3_output_mse: nan\n",
            "Starting training on batch of models for gamma values  [[0.7, 0.2, 0.1, 0, 0]] \n",
            "\n",
            "\n",
            "Training model for gamma equal to  [0.7, 0.2, 0.1, 0, 0]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-7f910cb892f0>\u001b[0m in \u001b[0;36m<cell line: 179>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m   \u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrained_models\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgamma_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_models\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"./trained_models_sixth_augmentation\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-35-7f910cb892f0>\u001b[0m in \u001b[0;36mfit_batch\u001b[0;34m(gamma_values, epochs, batch_size, save_models, models_dir, verbose)\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompile_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         model_history = model.fit({'input': x_train},\n\u001b[0m\u001b[1;32m    132\u001b[0m                             {'task_1_output': y_train_1, 'task_2_output': y_train_2, 'task_3_output': y_train_3,\n\u001b[1;32m    133\u001b[0m                              \u001b[0;31m#'task_4_output': y_train_4, 'task_5_output': y_train_5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1683\u001b[0m                         ):\n\u001b[1;32m   1684\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1685\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1686\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 894\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    895\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    924\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 926\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    927\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    141\u001b[0m       (concrete_function,\n\u001b[1;32m    142\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m--> 143\u001b[0;31m     return concrete_function._call_flat(\n\u001b[0m\u001b[1;32m    144\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1755\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1756\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1757\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1758\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1759\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    379\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    382\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Trying to improve multitask!!!\n",
        "\n",
        "#best result --> no augmentation\n",
        "\n",
        "\"\"\" Functions for creating, compiling and training model\"\"\"\n",
        "\n",
        "# Architecture:\n",
        "# Main branch\n",
        "# Branch1 => Decides wheter life threatening (VF or VT style) or not (SR and others)\n",
        "# Branch2 => Classify the input\n",
        "\n",
        "\n",
        "# labels,Rhythm\n",
        "# AFb,Atrial Fibrillation\n",
        "# AFt,Atrial Flutter\n",
        "# SR,Sinus Rhythm\n",
        "# SVT,Supraventricular Tachycardia\n",
        "# VFb,Ventricular Fibrillation\n",
        "# VFt,Ventricular Flutter\n",
        "# VPD,Ventricular Premature Depolarizations\n",
        "# VT,Ventricular Tachycardia\n",
        "\n",
        "\n",
        "def gen_model():\n",
        "    inputs = keras.layers.Input(shape=(1250, 1), name='input')\n",
        "\n",
        "    main_branch = keras.layers.Conv1D(filters=3, kernel_size=6, strides=2, activation=\"relu\") (inputs)\n",
        "    main_branch = keras.layers.BatchNormalization() (main_branch)\n",
        "    main_branch = keras.layers.Conv1D(filters=5, kernel_size=5, strides=2,  activation=\"relu\") (main_branch)\n",
        "    main_branch = keras.layers.BatchNormalization() (main_branch)\n",
        "    main_branch = keras.layers.Conv1D(filters=10, kernel_size=4, strides=2,  activation=\"relu\") (main_branch)\n",
        "    main_branch = keras.layers.BatchNormalization() (main_branch)\n",
        "    main_branch = keras.layers.Conv1D(filters=20, kernel_size=4, strides=2,  activation=\"relu\") (main_branch)\n",
        "    main_branch = keras.layers.BatchNormalization() (main_branch)\n",
        "    main_branch = keras.layers.Conv1D(filters=20, kernel_size=4, strides=2,  activation=\"relu\") (main_branch)\n",
        "    main_branch = keras.layers.BatchNormalization() (main_branch)\n",
        "    #main_branch = keras.layers.Conv1D(filters=32, kernel_size=1, strides=32, activation=\"relu\") (main_branch)\n",
        "    #main_branch = keras.layers.BatchNormalization() (main_branch)\n",
        "    main_branch = keras.layers.Flatten() (main_branch)\n",
        "    main_branch = keras.layers.Dropout(0.5) (main_branch)\n",
        "    #main_branch = keras.layers.Dense(512, activation=\"relu\") (main_branch)\n",
        "    #main_branch = keras.layers.Dropout(0.1) (main_branch)\n",
        "    #main_branch = keras.layers.Dense(512, activation=\"relu\") (main_branch)\n",
        "    #main_branch = keras.layers.Dropout(0.1) (main_branch)\n",
        "    main_branch = keras.layers.Dense(512, activation=\"relu\") (main_branch)\n",
        "    main_branch = keras.layers.BatchNormalization() (main_branch)\n",
        "\n",
        "    #branch1 = keras.layers.Dense(128, activation=\"relu\") (branch1)\n",
        "    #branch1 = keras.layers.BatchNormalization() (branch1)\n",
        "    #branch1 = keras.layers.Dense(128, activation=\"relu\") (branch1)\n",
        "    #branch1 = keras.layers.BatchNormalization() (branch1)\n",
        "    #branch1 = keras.layers.Dropout(0.2) (branch1)\n",
        "\n",
        "    # Decides wheter life threatening (VF or VT style) or not (SR and others)\n",
        "    branch1 = keras.layers.Dense(2, activation=\"softmax\", name='task_1_output') (main_branch)\n",
        "\n",
        "    #branch2 = keras.layers.Dense(128, activation=\"relu\") (main_branch)\n",
        "    #branch2 = keras.layers.BatchNormalization() (branch2)\n",
        "    #branch2 = keras.layers.Dense(128, activation=\"relu\") (branch2)\n",
        "    #branch2 = keras.layers.BatchNormalization() (branch2)\n",
        "    #branch2 = keras.layers.Dense(128, activation=\"relu\") (branch2)\n",
        "    #branch2 = keras.layers.BatchNormalization() (branch2)\n",
        "    #branch2 = keras.layers.Dropout(0.2) (branch2)\n",
        "\n",
        "    # Classify the input\n",
        "    branch2 = keras.layers.Dense(8, activation=\"softmax\", name='task_2_output') (main_branch)\n",
        "\n",
        "    # mean\n",
        "    branch3 = keras.layers.Dense(1, activation=\"relu\", name='task_3_output') (main_branch)\n",
        "\n",
        "    ## std\n",
        "    #branch4 = keras.layers.Dense(1, activation=\"relu\", name='task_4_output') (main_branch)\n",
        "#\n",
        "    ## var\n",
        "    #branch5 = keras.layers.Dense(1, activation=\"relu\", name='task_5_output') (main_branch)\n",
        "\n",
        "    model = tf.keras.Model(inputs = inputs, outputs = [branch1, branch2, branch3,]) #branch4, branch5])\n",
        "\n",
        "    return model\n",
        "\n",
        "# loss_weight -> [0, 1]\n",
        "# the weight for the second task is calculated by '1 - loss_weight'\n",
        "def compile_model(model, loss_weight1, loss_weight2, loss_weight3, loss_weight4, loss_weight5):\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(0.0002),\n",
        "                  loss={'task_1_output': 'binary_crossentropy',\n",
        "                        'task_2_output': 'categorical_crossentropy',\n",
        "                        'task_3_output': 'mse',\n",
        "                        #'task_4_output': 'mse',\n",
        "                        #'task_5_output': 'mse'\n",
        "                        },\n",
        "                  loss_weights={'task_1_output': loss_weight1,\n",
        "                                'task_2_output': loss_weight2,\n",
        "                                'task_3_output': loss_weight3,\n",
        "                                #'task_4_output': loss_weight4,\n",
        "                                #'task_5_output': loss_weight5\n",
        "                                },\n",
        "                  metrics={'task_1_output' : [\"accuracy\", tf.keras.metrics.Precision(), tf.keras.metrics.Recall()],\n",
        "                           'task_2_output' : [\"accuracy\", tf.keras.metrics.Precision(), tf.keras.metrics.Recall()],\n",
        "                           'task_3_output' : \"mse\",\n",
        "                           #'task_4_output' : \"mse\",\n",
        "                           #'task_5_output' : \"mse\"\n",
        "                           })\n",
        "    return model\n",
        "\n",
        "def fit_batch(gamma_values, epochs=30, batch_size=32, save_models=False, models_dir=\"./trained_models\", verbose=0):\n",
        "\n",
        "    history = list()\n",
        "    trained_models = list()\n",
        "    test_scores = list()\n",
        "\n",
        "    start_epoch = 10\n",
        "    swa = SWA(start_epoch=start_epoch,\n",
        "          lr_schedule='cyclic',\n",
        "          swa_lr=0.0001,\n",
        "          swa_lr2=0.0005,\n",
        "          swa_freq=5,\n",
        "          batch_size=batch_size,\n",
        "          verbose=1)\n",
        "\n",
        "    if save_models and not os.path.isdir(models_dir):\n",
        "      os.mkdir(models_dir)\n",
        "\n",
        "    print('Starting training on batch of models for gamma values ', gamma_values, '\\n\\n')\n",
        "\n",
        "    for i, gamma in enumerate(gamma_values):\n",
        "\n",
        "        print('Training model for gamma equal to ', gamma)\n",
        "        model = gen_model()\n",
        "        model = compile_model(model, gamma[0], gamma[1], gamma[2], gamma[3], gamma[4])\n",
        "        start = time.time()\n",
        "        model_history = model.fit({'input': x_train},\n",
        "                            {'task_1_output': y_train_1, 'task_2_output': y_train_2, 'task_3_output': y_train_3,\n",
        "                             #'task_4_output': y_train_4, 'task_5_output': y_train_5\n",
        "                             },\n",
        "                            epochs=epochs, batch_size=batch_size, verbose=verbose) # callbacks=[swa],\n",
        "        print(f'Training time: {time.time() - start}\\n')\n",
        "        history.append(model_history)\n",
        "        trained_models.append(model)\n",
        "        if save_models:\n",
        "          model.save(os.path.join(models_dir, f\"{i}th-model\"))\n",
        "\n",
        "        test_score = model.evaluate({'input': x_test}, {'task_1_output': y_test_1, 'task_2_output': y_test_2, 'task_3_output': y_test_3,\n",
        "                                                        #'task_4_output': y_test_4, 'task_5_output': y_test_5\n",
        "                                                        })\n",
        "        for metric, result in zip(model.metrics_names, test_score):\n",
        "          print(f\"{metric}: {result}\")\n",
        "\n",
        "        test_scores.append(test_score)\n",
        "\n",
        "    return history, trained_models, test_scores\n",
        "\n",
        "def plot_multitask_accuracies(gammas, training_history):\n",
        "\n",
        "    counter = 0\n",
        "\n",
        "    for history in training_history:\n",
        "\n",
        "        print(f'\\nPlotting Accuracy/Precision/Recall vs Epochs for value of gamma number {gammas[counter]}\\n')\n",
        "        plt.plot(range(len(history.history['task_1_output_accuracy'])), history.history['task_1_output_accuracy'], c='r', label='Task 1')\n",
        "        plt.plot(range(len(history.history['task_2_output_accuracy'])), history.history['task_2_output_accuracy'], c='b', label='Task 2')\n",
        "        plt.plot(range(len(history.history['task_1_output_precision'])), history.history['task_1_output_precision'], c='r', label='Task 1')\n",
        "        plt.plot(range(len(history.history['task_2_output_precision'])), history.history['task_2_output_precision'], c='b', label='Task 2')\n",
        "        plt.plot(range(len(history.history['task_1_output_recall'])), history.history['task_1_output_recall'], c='r', label='Task 1')\n",
        "        plt.plot(range(len(history.history['task_2_output_recall'])), history.history['task_2_output_recall'], c='b', label='Task 2')\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "        counter += 1\n",
        "\n",
        "\"\"\"Actually training the model\"\"\"\n",
        "#gamma_values = [ [0.2, 0.2, 0.2, 0.2, 0.2],\n",
        "#                [0.8, 0.1, 0.0333, 0.0333, 0.0333],\n",
        "#                [0.5, 0.125, 0.125, 0.125, 0.125],\n",
        "#                [0.5, 0.2, 0.1, 0.1, 0.1],\n",
        "#                [0.6, 0.3, 0.033, 0.033, 0.033]]\n",
        "\n",
        "gamma_values = [ [0.7, 0.2, 0.1, 0, 0],]\n",
        "\n",
        "for _ in range(20):\n",
        "  history, trained_models, _ = fit_batch(gamma_values, epochs=20, save_models=True, models_dir=\"./trained_models_sixth_augmentation\", verbose=0)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}